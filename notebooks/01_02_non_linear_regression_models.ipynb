{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    output_filepath\n",
    "except:\n",
    "    output_filepath = 's3://workspaces-clarity-mgmt-pro/jaime.oliver/misc/social_capital/data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/anaconda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from yellowbrick.regressor import ResidualsPlot, PredictionError\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(output_filepath, 'panel_data.parquet')\n",
    "df_model = pd.read_parquet(data_path)\n",
    "\n",
    "centralities = ['hubs', 'authorities', 'favor']\n",
    "networks = ['financial', 'goods', 'human']\n",
    "\n",
    "for c in [f'{n}_{c}' for c in centralities for n in networks]:\n",
    "    df_model[c] = df_model[c].map(lambda x: np.log1p(x*1.e8))\n",
    "\n",
    "df_model = df_model[~df_model.country.isin(['MNE', 'SRB', 'STP', 'PRK'])]\n",
    "\n",
    "df_model = df_model[df_model.year.between(1995, 2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centralities = [f'{n}_{c}' for c in centralities for n in networks]\n",
    "all_centralities.remove('goods_favor')\n",
    "all_centralities.remove('financial_favor')\n",
    "\n",
    "controls = ['log_GFCF', 'log_wkn_population', 'year', 'country']\n",
    "x_labels = all_centralities +  controls\n",
    "y_label = 'log_gdp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed: 12.3min\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model[x_labels], df_model[y_label], test_size=0.33, random_state=42, stratify=df_model[['country']] )\n",
    "\n",
    "# Transform\n",
    "# Impute numeric values with median \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median',add_indicator=True)),\n",
    "    ('transformer', FunctionTransformer(None, validate=False))])\n",
    "\n",
    "# Impute categorical features with 'missing' and one-hot-encode them\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', \n",
    "                              fill_value='missing',\n",
    "                              add_indicator=True)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine all transformers in a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, make_column_selector(dtype_include=[\"float64\", \"int64\"])),\n",
    "        ('cat', categorical_transformer, make_column_selector(dtype_include=[\"object\"])),])\n",
    "\n",
    "regressor = Pipeline(steps=[ ('preprocessor', preprocessor),\n",
    "                             ('GradientBoostingRegressor', GradientBoostingRegressor())])\n",
    "\n",
    "model_pipe = TransformedTargetRegressor(regressor, func=None, inverse_func=None)\n",
    "\n",
    "params = {'regressor__GradientBoostingRegressor__max_depth':[2,3,4],\n",
    "          'regressor__GradientBoostingRegressor__learning_rate':[0.5,0.1,0.05, 0.01],\n",
    "          'regressor__GradientBoostingRegressor__n_estimators':[int(n) for n in np.logspace(1,3.5,20)],\n",
    "          'regressor__GradientBoostingRegressor__subsample':[0.8],\n",
    "          'regressor__GradientBoostingRegressor__max_features':['auto']}\n",
    "model = GridSearchCV(model_pipe, param_grid=params, cv=5, verbose=1, n_jobs=4)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(model)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  \n",
    "visualizer.score(X_test, y_test) \n",
    "visualizer.show() \n",
    "\n",
    "save_to = os.path.join(Path(os.getcwd()).parent.resolve(), 'reports', 'figures', f'residuals_ml_model.png')\n",
    "plt.savefig(save_to)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = PredictionError(model)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  \n",
    "visualizer.score(X_test, y_test) \n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100\n",
    "X_set = X_test.sample(n_points)\n",
    "\n",
    "X_set = shap.sample(X_test, n_points)\n",
    "\n",
    "def model_predict(data_asarray):\n",
    "    data_asframe =  pd.DataFrame(data_asarray, columns=X_test.columns)\n",
    "    return model.best_estimator_.predict(data_asframe)\n",
    "\n",
    "explainer = shap.KernelExplainer(model_predict, data=X_set.values, feature_names = x_labels, algorithm = 'auto')\n",
    "shap_values = explainer.shap_values(X_set)\n",
    "\n",
    "shap.summary_plot(shap_values, X_set, show=False)\n",
    "\n",
    "\n",
    "save_to = os.path.join(Path(os.getcwd()).parent.resolve(), 'reports', 'figures', f'shap_values_levels.png')\n",
    "plt.savefig(save_to)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_all_terms_list = ['delta_financial_' + c for c in centralities] + ['delta_human_' + c for c in centralities]\n",
    "x_labels = ['lag_log_output', 'lag_log2_output', 'country', 'year'] + lagged_all_terms_list\n",
    "y_label = 'delta_log_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "df_model_no_nan = df_model.dropna(subset=[y_label])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_model_no_nan[x_labels], df_model_no_nan[y_label], test_size=0.33, random_state=42, stratify=df_model_no_nan[['country']] )\n",
    "\n",
    "# Transform\n",
    "# Impute numeric values with median \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median',add_indicator=True)),\n",
    "    ('transformer', FunctionTransformer(None, validate=False))])\n",
    "\n",
    "# Impute categorical features with 'missing' and one-hot-encode them\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', \n",
    "                              fill_value='missing',\n",
    "                              add_indicator=True)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine all transformers in a single preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, make_column_selector(dtype_include=[\"float64\", \"int64\"])),\n",
    "        ('cat', categorical_transformer, make_column_selector(dtype_include=[\"object\"])),])\n",
    "\n",
    "regressor = Pipeline(steps=[ ('preprocessor', preprocessor),\n",
    "                             ('GradientBoostingRegressor', GradientBoostingRegressor())])\n",
    "\n",
    "model_pipe = TransformedTargetRegressor(regressor, func=None, inverse_func=None)\n",
    "\n",
    "params = {'regressor__GradientBoostingRegressor__max_depth':[2,3,4],\n",
    "          'regressor__GradientBoostingRegressor__learning_rate':[0.5,0.1,0.05,0.01],\n",
    "          'regressor__GradientBoostingRegressor__n_estimators':[int(n) for n in np.logspace(1,3.5,20)],\n",
    "          'regressor__GradientBoostingRegressor__subsample':[0.8],\n",
    "          'regressor__GradientBoostingRegressor__max_features':['auto']}\n",
    "model = GridSearchCV(model_pipe, param_grid=params, cv=5, verbose=1, n_jobs=4)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(model)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  \n",
    "visualizer.score(X_test, y_test) \n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = PredictionError(model)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  \n",
    "visualizer.score(X_test, y_test) \n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 10\n",
    "X_set = X_test.sample(n_points)\n",
    "\n",
    "X_set = shap.sample(X_test, 10)\n",
    "\n",
    "def model_predict(data_asarray):\n",
    "    data_asframe =  pd.DataFrame(data_asarray, columns=X_test.columns)\n",
    "    return model.best_estimator_.predict(data_asframe)\n",
    "\n",
    "explainer = shap.KernelExplainer(model_predict, data=X_set.values, feature_names = x_labels, algorithm = 'auto')\n",
    "shap_values = explainer.shap_values(X_set)\n",
    "\n",
    "shap.summary_plot(shap_values, X_set, show=False)\n",
    "\n",
    "save_to = os.path.join(Path(output_filepath).parent.parent.resolve(), 'reports', 'figures', 'shap_values_time_series.png')\n",
    "plt.savefig(save_to)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Negative effect of time can be explained by a growth decline for the sample countries and time sample\n",
    "'''\n",
    "import seaborn as sns\n",
    "sns.lineplot(data=df_model, x='year', y='delta_log_output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
